{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬äºŒè¯¾ï¼šPrompt Template å’Œ Chain\n",
    "\n",
    "## å­¦ä¹ ç›®æ ‡\n",
    "1. äº†è§£ Prompt Templateï¼ˆæç¤ºè¯æ¨¡æ¿ï¼‰\n",
    "2. äº†è§£ Chainï¼ˆé“¾ï¼‰çš„æ¦‚å¿µ\n",
    "3. ä½¿ç”¨ LCEL (LangChain Expression Language) æ„å»ºå·¥ä½œæµ\n",
    "\n",
    "## ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ¦‚å¿µï¼Ÿ\n",
    "\n",
    "- **Prompt Template**ï¼šè®©æç¤ºè¯å¯å¤ç”¨ã€å¯å‚æ•°åŒ–\n",
    "- **Chain**ï¼šå°†å¤šä¸ªå¤„ç†æ­¥éª¤è¿æ¥èµ·æ¥ï¼Œå½¢æˆæµæ°´çº¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‡†å¤‡å·¥ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# è®¾ç½® API Keyï¼ˆæ›¿æ¢ä¸ºä½ çš„ Keyï¼‰\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = \"your_api_key_here\"\n",
    "\n",
    "def create_qwen_chat():\n",
    "    \"\"\"åˆ›å»ºåƒé—®èŠå¤©æ¨¡å‹\"\"\"\n",
    "    api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "    if not api_key or api_key == \"your_api_key_here\":\n",
    "        raise ValueError(\"âš ï¸ è¯·è®¾ç½®ä½ çš„ DASHSCOPE_API_KEY\")\n",
    "    \n",
    "    return ChatOpenAI(\n",
    "        model=\"qwen-plus\",\n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "print(\"âœ… å‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Prompt Templateï¼ˆæç¤ºè¯æ¨¡æ¿ï¼‰\n",
    "\n",
    "Prompt Template è®©ä½ å¯ä»¥ï¼š\n",
    "- å®šä¹‰å¯å¤ç”¨çš„æç¤ºè¯ç»“æ„\n",
    "- åŠ¨æ€æ’å…¥å˜é‡\n",
    "- ä¿æŒæç¤ºè¯çš„ä¸€è‡´æ€§\n",
    "\n",
    "### åŸºç¡€æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªç®€å•çš„æç¤ºè¯æ¨¡æ¿\n",
    "# {topic} æ˜¯ä¸€ä¸ªå˜é‡å ä½ç¬¦\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ€æœ¯ä¸“å®¶ã€‚è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šä»€ä¹ˆæ˜¯ {topic}ï¼Œ\"\n",
    "    \"å¹¶ç»™å‡ºä¸€ä¸ªå®é™…åº”ç”¨çš„ä¾‹å­ã€‚å›ç­”æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚\"\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹æ¨¡æ¿çš„å˜é‡\n",
    "print(f\"æ¨¡æ¿å˜é‡: {prompt.input_variables}\")\n",
    "\n",
    "# æ ¼å¼åŒ–æ¨¡æ¿ - å°†å˜é‡æ›¿æ¢ä¸ºå®é™…å€¼\n",
    "formatted = prompt.format(topic=\"æœºå™¨å­¦ä¹ \")\n",
    "print(f\"\\næ ¼å¼åŒ–åçš„æç¤ºè¯:\\n{formatted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨æ¨¡æ¿è°ƒç”¨æ¨¡å‹\n",
    "chat = create_qwen_chat()\n",
    "\n",
    "# format_messages è¿”å›æ¶ˆæ¯åˆ—è¡¨\n",
    "messages = prompt.format_messages(topic=\"æœºå™¨å­¦ä¹ \")\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "print(\"é—®é¢˜: ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ\")\n",
    "print(f\"\\nå›å¤:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤ç”¨åŒä¸€ä¸ªæ¨¡æ¿ï¼Œè§£é‡Šä¸åŒçš„æ¦‚å¿µ\n",
    "topics = [\"åŒºå—é“¾\", \"äº‘è®¡ç®—\", \"äººå·¥æ™ºèƒ½\"]\n",
    "\n",
    "for topic in topics:\n",
    "    response = chat.invoke(prompt.format_messages(topic=topic))\n",
    "    print(f\"\\nğŸ“š {topic}:\")\n",
    "    print(f\"{response.content}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### èŠå¤©æç¤ºè¯æ¨¡æ¿\n",
    "\n",
    "`ChatPromptTemplate` æ”¯æŒå¤šç§æ¶ˆæ¯ç±»å‹ï¼š\n",
    "- `system`: ç³»ç»Ÿæ¶ˆæ¯\n",
    "- `human`: ç”¨æˆ·æ¶ˆæ¯\n",
    "- `ai`: AI æ¶ˆæ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¸¦æœ‰ç³»ç»Ÿæ¶ˆæ¯çš„èŠå¤©æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ª{role}ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£ç­”{domain}ç›¸å…³çš„é—®é¢˜ã€‚å›ç­”è¦ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚ã€‚\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "print(f\"æ¨¡æ¿å˜é‡: {prompt.input_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨æ¨¡æ¿\n",
    "messages = prompt.format_messages(\n",
    "    role=\"Python å¯¼å¸ˆ\",\n",
    "    domain=\"Python ç¼–ç¨‹\",\n",
    "    question=\"ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿç®€å•ä¸¾ä¸ªä¾‹å­\"\n",
    ")\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "print(\"è§’è‰²: Python å¯¼å¸ˆ\")\n",
    "print(\"é—®é¢˜: ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ\")\n",
    "print(f\"\\nå›å¤:\\n{response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Chainï¼ˆé“¾ï¼‰å’Œ LCEL\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯ LCELï¼Ÿ\n",
    "\n",
    "**LCEL (LangChain Expression Language)** ä½¿ç”¨ `|` æ“ä½œç¬¦å°†å¤šä¸ªç»„ä»¶è¿æ¥æˆä¸€ä¸ªé“¾ï¼š\n",
    "\n",
    "```\n",
    "prompt | model | output_parser\n",
    "```\n",
    "\n",
    "è¿™ç±»ä¼¼äº Unix ç®¡é“ï¼Œæ•°æ®ä»å·¦åˆ°å³æµåŠ¨ï¼š\n",
    "1. `prompt` æ¥æ”¶è¾“å…¥ï¼Œç”Ÿæˆæç¤ºè¯\n",
    "2. `model` æ¥æ”¶æç¤ºè¯ï¼Œç”Ÿæˆå›å¤\n",
    "3. `output_parser` å¤„ç†å›å¤ï¼Œè¿”å›æœ€ç»ˆç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# åˆ›å»ºç»„ä»¶\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{language}ï¼š\\n\\n{text}\"\n",
    ")\n",
    "model = create_qwen_chat()\n",
    "output_parser = StrOutputParser()  # å°† AIMessage è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "\n",
    "# ä½¿ç”¨ LCEL åˆ›å»ºé“¾\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "print(\"âœ… ç¿»è¯‘é“¾åˆ›å»ºå®Œæˆ\")\n",
    "print(\"\\né“¾çš„ç»“æ„: prompt â†’ model â†’ output_parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è°ƒç”¨é“¾\n",
    "result = chain.invoke({\n",
    "    \"language\": \"è‹±æ–‡\",\n",
    "    \"text\": \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚\"\n",
    "})\n",
    "\n",
    "print(\"åŸæ–‡: äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚\")\n",
    "print(f\"ç¿»è¯‘ (è‹±æ–‡): {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŒä¸€ä¸ªé“¾ï¼Œç¿»è¯‘æˆä¸åŒè¯­è¨€\n",
    "languages = [(\"æ—¥è¯­\", \"ä»Šå¤©å¤©æ°”çœŸå¥½ï¼\"), (\"æ³•è¯­\", \"æˆ‘çˆ±ç¼–ç¨‹\"), (\"éŸ©è¯­\", \"è°¢è°¢ä½ çš„å¸®åŠ©\")]\n",
    "\n",
    "for lang, text in languages:\n",
    "    result = chain.invoke({\"language\": lang, \"text\": text})\n",
    "    print(f\"{text} â†’ ({lang}) {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤šæ­¥éª¤é“¾\n",
    "\n",
    "å¯ä»¥åˆ›å»ºå¤šä¸ªé“¾ï¼Œç„¶åç»„åˆå®ƒä»¬å®Œæˆå¤æ‚ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ•…äº‹å¤§çº²\n",
    "outline_prompt = ChatPromptTemplate.from_template(\n",
    "    \"è¯·ä¸ºä¸€ä¸ªå…³äº{theme}çš„çŸ­æ•…äº‹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„å¤§çº²ï¼ˆ3ä¸ªè¦ç‚¹ï¼‰ã€‚\"\n",
    ")\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šæ ¹æ®å¤§çº²å†™æ•…äº‹\n",
    "story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"æ ¹æ®ä»¥ä¸‹å¤§çº²ï¼Œå†™ä¸€ä¸ª100å­—å·¦å³çš„çŸ­æ•…äº‹ï¼š\\n\\n{outline}\"\n",
    ")\n",
    "\n",
    "# åˆ›å»ºä¸¤ä¸ªé“¾\n",
    "outline_chain = outline_prompt | model | StrOutputParser()\n",
    "story_chain = story_prompt | model | StrOutputParser()\n",
    "\n",
    "print(\"âœ… å¤šæ­¥éª¤é“¾åˆ›å»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œå¤šæ­¥éª¤ä»»åŠ¡\n",
    "theme = \"ä¸€åªå‹‡æ•¢çš„å°çŒ«\"\n",
    "print(f\"ğŸ± ä¸»é¢˜: {theme}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²\n",
    "print(\"\\nğŸ“‹ ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²\")\n",
    "outline = outline_chain.invoke({\"theme\": theme})\n",
    "print(outline)\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šæ ¹æ®å¤§çº²å†™æ•…äº‹\n",
    "print(\"\\nğŸ“– ç¬¬äºŒæ­¥ï¼šå†™æ•…äº‹\")\n",
    "story = story_chain.invoke({\"outline\": outline})\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassthrough å’Œ RunnableParallel\n",
    "\n",
    "- `RunnablePassthrough`: åŸæ ·ä¼ é€’è¾“å…¥\n",
    "- `RunnableParallel`: å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ“ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªåŒæ—¶è¿”å›åŸæ–‡å’Œç¿»è¯‘çš„é“¾\n",
    "translate_prompt = ChatPromptTemplate.from_template(\n",
    "    \"å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\\n{text}\"\n",
    ")\n",
    "\n",
    "# RunnableParallel å…è®¸å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ“ä½œ\n",
    "chain = RunnableParallel(\n",
    "    original=RunnablePassthrough(),  # ä¼ é€’åŸå§‹è¾“å…¥\n",
    "    translated=translate_prompt | model | StrOutputParser()  # ç¿»è¯‘\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"text\": \"å­¦ä¹ ç¼–ç¨‹å¾ˆæœ‰è¶£ï¼\"})\n",
    "\n",
    "print(f\"åŸæ–‡: {result['original']['text']}\")\n",
    "print(f\"ç¿»è¯‘: {result['translated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. å®æˆ˜ï¼šä»£ç å®¡æŸ¥åŠ©æ‰‹\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå®ç”¨çš„ä»£ç å®¡æŸ¥å·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä»£ç å®¡æŸ¥æç¤ºè¯æ¨¡æ¿\n",
    "review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚\n",
    "è¯·å®¡æŸ¥ç”¨æˆ·æä¾›çš„ä»£ç ï¼Œä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ç»™å‡ºå»ºè®®ï¼š\n",
    "1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§\n",
    "2. æ½œåœ¨çš„ bug æˆ–é—®é¢˜\n",
    "3. æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "4. æœ€ä½³å®è·µ\n",
    "\n",
    "è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚\"\"\"),\n",
    "    (\"human\", \"è¯·å®¡æŸ¥ä»¥ä¸‹ {language} ä»£ç ï¼š\\n\\n```{language}\\n{code}\\n```\")\n",
    "])\n",
    "\n",
    "# åˆ›å»ºå®¡æŸ¥é“¾\n",
    "review_chain = review_prompt | model | StrOutputParser()\n",
    "\n",
    "print(\"âœ… ä»£ç å®¡æŸ¥åŠ©æ‰‹åˆ›å»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¾…å®¡æŸ¥çš„ä»£ç \n",
    "test_code = '''\n",
    "def find_user(users, name):\n",
    "    for i in range(len(users)):\n",
    "        if users[i][\"name\"] == name:\n",
    "            return users[i]\n",
    "    return None\n",
    "\n",
    "def get_user_emails(users):\n",
    "    emails = []\n",
    "    for user in users:\n",
    "        emails.append(user[\"email\"])\n",
    "    return emails\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“ å¾…å®¡æŸ¥çš„ä»£ç :\")\n",
    "print(test_code)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œä»£ç å®¡æŸ¥\n",
    "review = review_chain.invoke({\n",
    "    \"language\": \"python\",\n",
    "    \"code\": test_code\n",
    "})\n",
    "\n",
    "print(\"ğŸ” å®¡æŸ¥ç»“æœ:\")\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ç»ƒä¹ \n",
    "\n",
    "å°è¯•åˆ›å»ºè‡ªå·±çš„é“¾æ¥å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  1ï¼šåˆ›å»ºä¸€ä¸ªæ–‡ç« æ‘˜è¦é“¾\n",
    "# è¾“å…¥ä¸€ç¯‡æ–‡ç« ï¼Œè¾“å‡º3å¥è¯çš„æ‘˜è¦\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"è¯·ç”¨3å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡ç« çš„è¦ç‚¹ï¼š\\n\\n{article}\"\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | model | StrOutputParser()\n",
    "\n",
    "# æµ‹è¯•\n",
    "article = \"\"\"\n",
    "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚ä»æ™ºèƒ½æ‰‹æœºä¸Šçš„è¯­éŸ³åŠ©æ‰‹ï¼Œåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ\n",
    "å†åˆ°åŒ»ç–—è¯Šæ–­ç³»ç»Ÿï¼ŒAIçš„åº”ç”¨å·²ç»æ¸—é€åˆ°å„ä¸ªé¢†åŸŸã€‚éšç€å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼ŒAIç°åœ¨èƒ½å¤Ÿè¿›è¡Œå¤æ‚çš„\n",
    "å¯¹è¯ã€å†™ä½œå’Œç¼–ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®éšç§ã€å°±ä¸šå½±å“å’Œä¼¦ç†é—®é¢˜ã€‚ä¸“å®¶ä»¬è®¤ä¸ºï¼Œ\n",
    "æˆ‘ä»¬éœ€è¦åœ¨æ¨åŠ¨æŠ€æœ¯å‘å±•çš„åŒæ—¶ï¼Œå»ºç«‹ç›¸åº”çš„ç›‘ç®¡æ¡†æ¶ï¼Œç¡®ä¿AIçš„å‘å±•é€ ç¦äººç±»ã€‚\n",
    "\"\"\"\n",
    "\n",
    "summary = summary_chain.invoke({\"article\": article})\n",
    "print(\"ğŸ“„ æ–‡ç« æ‘˜è¦:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»ƒä¹  2ï¼šåˆ›å»ºä¸€ä¸ªé—®ç­”é“¾\n",
    "# å…ˆç»™å‡ºèƒŒæ™¯çŸ¥è¯†ï¼Œç„¶åå›ç­”é—®é¢˜\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„èƒŒæ™¯çŸ¥è¯†å›ç­”é—®é¢˜ï¼Œå¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚\"),\n",
    "    (\"human\", \"èƒŒæ™¯çŸ¥è¯†ï¼š\\n{context}\\n\\né—®é¢˜ï¼š{question}\")\n",
    "])\n",
    "\n",
    "qa_chain = qa_prompt | model | StrOutputParser()\n",
    "\n",
    "# æµ‹è¯•\n",
    "context = \"LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ LLM åº”ç”¨çš„æ¡†æ¶ã€‚å®ƒæä¾›äº† Prompt Templateã€Chainã€Agent ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚LangGraph æ˜¯ LangChain å›¢é˜Ÿå¼€å‘çš„ï¼Œç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„æ™ºèƒ½ä½“å·¥ä½œæµã€‚\"\n",
    "\n",
    "questions = [\n",
    "    \"LangChain æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"LangGraph æ˜¯è°å¼€å‘çš„ï¼Ÿ\",\n",
    "    \"TensorFlow æ˜¯ä»€ä¹ˆï¼Ÿ\"  # èƒŒæ™¯çŸ¥è¯†ä¸­æ²¡æœ‰çš„\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = qa_chain.invoke({\"context\": context, \"question\": q})\n",
    "    print(f\"â“ {q}\")\n",
    "    print(f\"ğŸ’¡ {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ æœ¬è¯¾å°ç»“\n",
    "\n",
    "| æ¦‚å¿µ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| `ChatPromptTemplate` | èŠå¤©æç¤ºè¯æ¨¡æ¿ï¼Œæ”¯æŒå˜é‡æ›¿æ¢ |\n",
    "| `from_template()` | ä»å­—ç¬¦ä¸²åˆ›å»ºæ¨¡æ¿ |\n",
    "| `from_messages()` | ä»æ¶ˆæ¯åˆ—è¡¨åˆ›å»ºæ¨¡æ¿ |\n",
    "| `LCEL (\\|)` | ä½¿ç”¨ç®¡é“æ“ä½œç¬¦è¿æ¥ç»„ä»¶ |\n",
    "| `StrOutputParser` | å°† AI å›å¤è½¬æ¢ä¸ºå­—ç¬¦ä¸² |\n",
    "| `RunnablePassthrough` | åŸæ ·ä¼ é€’è¾“å…¥ |\n",
    "| `RunnableParallel` | å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæ“ä½œ |\n",
    "\n",
    "## â¡ï¸ ä¸‹ä¸€è¯¾\n",
    "\n",
    "ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ç»™ AI æ·»åŠ **å·¥å…·ï¼ˆToolsï¼‰**èƒ½åŠ›ï¼Œè®©å®ƒèƒ½å¤Ÿè·å–å®æ—¶ä¿¡æ¯ã€æ‰§è¡Œè®¡ç®—ç­‰æ“ä½œï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
